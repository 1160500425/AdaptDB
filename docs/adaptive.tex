\documentclass[18pt]{article}
\usepackage{mathtools}
\begin{document}
\title{Adaptive Workload Partitioning}
\maketitle

This document describes the first cut ideas we have for implementing adaptive data  partitioning(ADP). We focus on how to handle ADP for single predicate select queries and later show how to generalize it to multiple predicates. In the end, some of the challenges which are yet to be solved are discussed.

The upfront partitioning step creates an initial index tree which has uniform $coverage$ for each attribute. The index tree is basically a balanced binary search tree where each node stores the dimension it partitions one, the cutpoint ($\leq$ cutpoint goes to left of tree), the left and right nodes it links to and its parent. The leaf nodes are buckets, each bucket has an id and represents a set of tuples which satisfy all the properties of nodes traversing upwards from the bucket to the root of the tree. 

In order to get a set of possible plans for re-partitioning the data, we define a set of transformation rules. When a new select query $A_p$ filtering on attribute $A$ with predicate $p$ comes in, all the plans we consider try to replace an existing node with a node partitioning on attribute $A$. We do not have to consider partitioning on any other attribute. (If we did have to consider, it means there was a query before on that attribute. We would have considered it at that point). In the discussion below, we assume that the predicate is $\leq p$. The other cases $\geq p$ and $== p$ can be handled in a similar manner.

All the re-partitioning plans are constrained to access only the data to be read by the input query ie: we cannot access data not read by the query during re-partitioning. This has two benefits: we never spend any effort in re-partitioning data that will not be touched by any query and more importantly, the query and re-partition can share the scan thereby reducing the cost of re-partitioning.

\section{Rules}

\textbf{Notation}: $X$ is a node in the index tree. $- X -$ means that both sides of the node are accessed ie: no filtering, $- X$ means only the left side is accessed. $- {A_p}$ means only the left subtree is accessed, implicitly saying that we have a query on A with predicate $q$ where $q <= p$. The arrow heads indicate the query.

The first two basic rules are 
\begin{eqnarray}
- X - \xrightarrow{A_p} - A_p \label{rule1} \\
- A_{p'}  \xrightarrow{A_p} - A_p  \label{rule2}
\end{eqnarray}

 Each rule application creates an alternative which will be evaluated independently. The above two rules are applied at the level above leaf nodes. If an attribute is accessed repeatedly, we try to push it higher up in the partitioning tree. This is kind-off similar to the idea of splay trees, as we try to push every attribute to the top of the tree, the more popular ones end up occupying the nodes higher up in the tree. We use the two rules below to push a node up in the tree:
\begin{eqnarray}
 (- A_p) X (- A_p) \xrightarrow{A_p} (- X -) A_p (X) \label{rule3} \\
 (- A_{p'}) X (- A_{p''}) \xrightarrow{A_p} (- A_{p})  A_{min(p',p'')} ( X ) \label{rule4}
\end{eqnarray}

Note that rule \ref{rule3}, \ref{rule4} both push $A$ higher up in the index tree but only rule \ref{rule4} increases the granularity of partitioning of $A$. 

Let us consider a simple example: Say we had a query $A_p$ and tree $(- A_{p'} ) X (- Z -)$. On applying the rule \ref{rule1}, we generate an alternative plan with $- Z -$ replaced by $- A_p$. We can also apply rule \ref{rule2} on the left subtree followed by \ref{rule3} at X to generate another alternative. 

While doing the transformations at higher levels in the tree, we still use just the data read by the query. However the partitioning effort is greater. 

\section{Costing}

The goal is to improve cost of executing the workload.
The optimizer contains a window of past queries. For each alternatives, we can find the improvement/deterioration in performance of each query measured by number of tuples accessed. For this, having the initial index tree, we create a shadow tree which has the nodes after transformation. The query predicates are used to find the blocks that will be accessed and hence the number of tuples accessed. 
Whether we combine the gains additively or with exponential weights to give more importance to recent queries has not been decided yet. For the alternative which has the highest gain, if this gain compensates for the write overhead - we move forward with the re-partitioning.

\section{Multiple Predicates}

For multiple predicates, the situation becomes more complex. The complexity comes from the fact that we can introduce the attribute from predicate 1, may or may not introduce attribute from predicate 2, etc. 

At the moment, we consider examining plans independently for each predicate and then choosing the best plan. This will make progress, however it might not work well for the following reasons:
\begin{itemize}
\item Consider a range query $45 <= A <= 55$ for an attribute A with uniform distribution of values from $1$ to $100$. Independently each predicate has $< 0.5$ selectivity. However together they have $0.1$ selectivity. Hence partitioning on either of them may not reduce the total number of tuples accessed by a large enough amount to make the plan feasible. However, not that it will eventually achieve good partitioning.
\item If we have two predicates one on A and one on B, if node partitioning on B is a parent of node partitioning on A, then we cannot push $A$ higher up in the tree even though it might be beneficial.
\end{itemize}
 
\end{document}